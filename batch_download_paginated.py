#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script para descargar im√°genes navegando p√°gina por p√°gina en resultados de b√∫squeda
Extrae links de propiedades de cada p√°gina y descarga sus im√°genes
"""
import os
import sys
import io
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import re

# Importar la clase de descarga
sys.path.insert(0, os.path.dirname(__file__))
from download_property_images import PropertyImageDownloader


class PaginatedPropertyScraper:
    """Descarga im√°genes navegando p√°gina por p√°gina en resultados de b√∫squeda"""

    def __init__(self, output_dir="property_images"):
        self.output_dir = output_dir
        self.downloader = PropertyImageDownloader(download_dir=output_dir)
        self.base_url = "https://bohioconsultores.com"

        # URLs de b√∫squeda disponibles
        self.search_urls = {
            'venta': "https://bohioconsultores.com/resultados-de-busqueda/?Servicio=2",
            'arriendo': "https://bohioconsultores.com/resultados-de-busqueda/?Servicio=1",
            'todas': "https://bohioconsultores.com/resultados-de-busqueda/"
        }

        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

    def fetch_page(self, url, retry=3):
        """Obtiene una p√°gina con reintentos"""
        for attempt in range(retry):
            try:
                print(f"\nüì• Obteniendo: {url}")
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                print(f"‚ö†Ô∏è  Intento {attempt + 1}/{retry} fall√≥: {e}")
                if attempt < retry - 1:
                    time.sleep(2)
                else:
                    print(f"‚ùå Error despu√©s de {retry} intentos")
                    return None

    def extract_property_links_from_page(self, html):
        """
        Extrae todos los enlaces de propiedades desde una p√°gina de resultados
        """
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        property_links = []

        # Buscar enlaces que contengan "detalle-propiedad"
        for link in soup.find_all('a', href=True):
            href = link['href']

            # Filtrar solo enlaces de detalle de propiedad
            if 'detalle-propiedad' in href:
                # Construir URL completa
                full_url = urljoin(self.base_url, href)

                # Evitar duplicados
                if full_url not in property_links:
                    property_links.append(full_url)

        return property_links

    def extract_next_page_link(self, html):
        """
        Extrae el enlace a la siguiente p√°gina de resultados
        Busca botones de paginaci√≥n, enlaces "Siguiente", etc.
        """
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # Estrategia 1: Buscar enlace con texto "Siguiente", "Next", ">"
        patterns = ['siguiente', 'next', '‚Ä∫', '¬ª', '‚Üí']
        for pattern in patterns:
            link = soup.find('a', string=re.compile(pattern, re.IGNORECASE))
            if link and link.get('href'):
                return urljoin(self.base_url, link['href'])

        # Estrategia 2: Buscar en clases comunes de paginaci√≥n
        pagination_classes = ['pagination', 'pager', 'page-numbers', 'nav-links']
        for class_name in pagination_classes:
            pagination = soup.find(class_=re.compile(class_name, re.IGNORECASE))
            if pagination:
                # Buscar el √∫ltimo enlace activo (usualmente es "siguiente")
                links = pagination.find_all('a', href=True)
                if links:
                    # Tomar el √∫ltimo enlace
                    last_link = links[-1]
                    return urljoin(self.base_url, last_link['href'])

        # Estrategia 3: Buscar par√°metros de p√°gina en URL
        # Ejemplo: ?page=2, ?p=2, ?pag=2
        current_page_match = re.search(r'[?&](page|p|pag|pagina)=(\d+)', html)
        if current_page_match:
            param_name = current_page_match.group(1)
            current_page = int(current_page_match.group(2))
            next_page = current_page + 1

            # Buscar URL base
            base_search = soup.find('form', {'action': re.compile('resultados-de-busqueda')})
            if base_search:
                action = base_search.get('action', '')
                return f"{action}?{param_name}={next_page}"

        return None

    def detect_pagination_info(self, html):
        """
        Detecta informaci√≥n de paginaci√≥n: p√°gina actual, total de p√°ginas, etc.
        """
        if not html:
            return {}

        soup = BeautifulSoup(html, 'html.parser')
        info = {
            'current_page': 1,
            'total_pages': None,
            'total_results': None
        }

        # Buscar texto como "P√°gina 2 de 10" o "Resultados 1-20 de 145"
        text_patterns = [
            r'p[a√°]gina\s+(\d+)\s+de\s+(\d+)',
            r'page\s+(\d+)\s+of\s+(\d+)',
            r'(\d+)\s+de\s+(\d+)\s+p[a√°]ginas',
            r'resultados?\s+\d+-\d+\s+de\s+(\d+)',
        ]

        page_text = soup.get_text()
        for pattern in text_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                groups = match.groups()
                if len(groups) >= 2:
                    info['current_page'] = int(groups[0])
                    info['total_pages'] = int(groups[1])
                elif len(groups) == 1:
                    info['total_results'] = int(groups[0])
                break

        return info

    def scrape_all_pages(self, search_type='venta', max_pages=None, delay=2):
        """
        Navega todas las p√°ginas de resultados y extrae enlaces de propiedades

        Args:
            search_type: 'venta', 'arriendo', o 'todas'
            max_pages: N√∫mero m√°ximo de p√°ginas a procesar (None = todas)
            delay: Segundos de espera entre p√°ginas
        """
        print("\n" + "="*80)
        print("üîç EXTRACTOR DE PROPIEDADES - NAVEGACI√ìN PAGINADA")
        print("="*80)

        # Obtener URL de b√∫squeda
        search_url = self.search_urls.get(search_type, self.search_urls['venta'])
        print(f"\nüéØ Tipo de b√∫squeda: {search_type.upper()}")
        print(f"üìç URL base: {search_url}")

        all_property_links = []
        current_url = search_url
        page_number = 1

        while current_url:
            print(f"\n{'='*80}")
            print(f"üìÑ P√ÅGINA {page_number}")
            print(f"{'='*80}")

            # Obtener p√°gina
            html = self.fetch_page(current_url)
            if not html:
                print(f"‚ùå No se pudo obtener la p√°gina {page_number}")
                break

            # Detectar informaci√≥n de paginaci√≥n
            pagination_info = self.detect_pagination_info(html)
            if pagination_info.get('total_pages'):
                print(f"üìä P√°gina {pagination_info['current_page']} de {pagination_info['total_pages']}")
            if pagination_info.get('total_results'):
                print(f"üìä Total resultados: {pagination_info['total_results']}")

            # Extraer enlaces de propiedades
            property_links = self.extract_property_links_from_page(html)
            print(f"üè† Propiedades encontradas en esta p√°gina: {len(property_links)}")

            # Agregar a lista total (evitar duplicados)
            new_links = [link for link in property_links if link not in all_property_links]
            all_property_links.extend(new_links)
            print(f"‚úÖ Nuevas propiedades: {len(new_links)}")
            print(f"üìä Total acumulado: {len(all_property_links)}")

            # Verificar si continuar
            if max_pages and page_number >= max_pages:
                print(f"\n‚è∏Ô∏è  L√≠mite de p√°ginas alcanzado: {max_pages}")
                break

            # Buscar siguiente p√°gina
            next_url = self.extract_next_page_link(html)
            if next_url:
                print(f"\n‚û°Ô∏è  Siguiente p√°gina encontrada")
                current_url = next_url
                page_number += 1

                # Esperar antes de siguiente p√°gina
                print(f"‚è±Ô∏è  Esperando {delay} segundos...")
                time.sleep(delay)
            else:
                print(f"\nüèÅ No hay m√°s p√°ginas")
                break

        print("\n" + "="*80)
        print("üìä RESUMEN DE EXTRACCI√ìN")
        print("="*80)
        print(f"   P√°ginas procesadas: {page_number}")
        print(f"   Total propiedades encontradas: {len(all_property_links)}")
        print("="*80)

        return all_property_links

    def download_from_links(self, property_links, max_properties=None, start_from=0, delay=1):
        """
        Descarga im√°genes de una lista de enlaces de propiedades
        """
        print("\n" + "="*80)
        print("üì• DESCARGA DE IM√ÅGENES")
        print("="*80)

        # Aplicar l√≠mites
        if start_from > 0:
            property_links = property_links[start_from:]
            print(f"üîÑ Iniciando desde posici√≥n: {start_from}")

        if max_properties:
            property_links = property_links[:max_properties]
            print(f"üî¢ Limitando a: {max_properties} propiedades")

        print(f"\nüöÄ Procesando {len(property_links)} propiedades...")

        # Estad√≠sticas
        stats = {
            'total': len(property_links),
            'success': 0,
            'failed': 0,
            'no_images': 0,
            'total_images': 0,
            'failed_urls': []
        }

        # Procesar cada enlace
        for idx, url in enumerate(property_links, 1):
            print(f"\n{'='*80}")
            print(f"[{idx}/{len(property_links)}] Procesando propiedad")
            print(f"{'='*80}")
            print(f"üîó URL: {url}")

            try:
                # Descargar im√°genes
                result = self.downloader.process_property_url(url, download_locally=True)

                if result and result.get('downloaded_images'):
                    stats['success'] += 1
                    stats['total_images'] += len(result['downloaded_images'])
                    print(f"‚úÖ C√≥digo {result['property_id']}: {len(result['downloaded_images'])} im√°genes")
                elif result:
                    stats['no_images'] += 1
                    print(f"‚ö†Ô∏è  C√≥digo {result.get('property_id', 'N/A')}: Sin im√°genes")
                else:
                    stats['failed'] += 1
                    stats['failed_urls'].append(url)
                    print(f"‚ùå No se pudo procesar")

            except Exception as e:
                stats['failed'] += 1
                stats['failed_urls'].append(url)
                print(f"‚ùå Error: {e}")

            # Pausa entre propiedades
            if idx < len(property_links):
                time.sleep(delay)

        # Reporte final
        print("\n" + "="*80)
        print("üìä REPORTE FINAL")
        print("="*80)
        print(f"   Total Procesadas: {stats['total']}")
        print(f"   ‚úÖ Exitosas: {stats['success']}")
        print(f"   ‚ùå Fallidas: {stats['failed']}")
        print(f"   ‚ö†Ô∏è  Sin im√°genes: {stats['no_images']}")
        print(f"   üì∏ Total Im√°genes Descargadas: {stats['total_images']}")

        if stats['failed_urls']:
            print(f"\nüìã URLs Fallidas ({len(stats['failed_urls'])}):")
            for i, url in enumerate(stats['failed_urls'][:10], 1):
                print(f"   {i:2d}. {url}")
            if len(stats['failed_urls']) > 10:
                print(f"   ... y {len(stats['failed_urls']) - 10} m√°s")

        print("="*80)

        return stats

    def run_full_process(self, search_type='venta', max_pages=None, max_properties=None,
                         start_from=0, page_delay=2, download_delay=1):
        """
        Proceso completo: navega p√°ginas y descarga im√°genes
        """
        print("\n" + "="*80)
        print("üöÄ PROCESO COMPLETO DE DESCARGA")
        print("="*80)
        print(f"\n‚öôÔ∏è  Configuraci√≥n:")
        print(f"   Tipo de b√∫squeda: {search_type}")
        print(f"   M√°x. p√°ginas: {max_pages or 'Todas'}")
        print(f"   M√°x. propiedades: {max_properties or 'Todas'}")
        print(f"   Iniciar desde: {start_from}")
        print(f"   Delay entre p√°ginas: {page_delay}s")
        print(f"   Delay entre descargas: {download_delay}s")

        # Fase 1: Extraer enlaces
        property_links = self.scrape_all_pages(
            search_type=search_type,
            max_pages=max_pages,
            delay=page_delay
        )

        if not property_links:
            print("\n‚ùå No se encontraron propiedades")
            return None

        # Guardar enlaces en archivo
        links_file = os.path.join(self.output_dir, f"property_links_{search_type}.txt")
        os.makedirs(self.output_dir, exist_ok=True)
        with open(links_file, 'w', encoding='utf-8') as f:
            for link in property_links:
                f.write(f"{link}\n")
        print(f"\nüíæ Enlaces guardados en: {links_file}")

        # Fase 2: Descargar im√°genes
        stats = self.download_from_links(
            property_links=property_links,
            max_properties=max_properties,
            start_from=start_from,
            delay=download_delay
        )

        print("\n" + "="*80)
        print("‚úÖ PROCESO COMPLETADO")
        print("="*80)

        return {
            'property_links': property_links,
            'stats': stats
        }


def main():
    """Funci√≥n principal"""

    print("="*80)
    print("DESCARGADOR PAGINADO - BOHIO CONSULTORES")
    print("="*80)

    # Configuraci√≥n
    search_type = 'venta'  # 'venta', 'arriendo', 'todas'
    max_pages = None  # None = todas las p√°ginas
    max_properties = None  # None = todas las propiedades
    start_from = 0  # √çndice de inicio

    # Parsear argumentos
    if len(sys.argv) > 1:
        try:
            # Argumento 1: tipo de b√∫squeda
            if sys.argv[1] in ['venta', 'arriendo', 'todas']:
                search_type = sys.argv[1]
            else:
                max_pages = int(sys.argv[1])
        except:
            print("üí° USO:")
            print("   python batch_download_paginated.py [tipo] [max_paginas] [max_propiedades]")
            print("\nüí° EJEMPLOS:")
            print("   python batch_download_paginated.py venta 5 20")
            print("   python batch_download_paginated.py arriendo 10")
            print("   python batch_download_paginated.py todas")
            print("\nüìù Tipos: venta, arriendo, todas")
            return

    if len(sys.argv) > 2:
        try:
            max_pages = int(sys.argv[2])
        except:
            pass

    if len(sys.argv) > 3:
        try:
            max_properties = int(sys.argv[3])
        except:
            pass

    # Crear scraper
    scraper = PaginatedPropertyScraper()

    # Ejecutar proceso completo
    scraper.run_full_process(
        search_type=search_type,
        max_pages=max_pages,
        max_properties=max_properties,
        start_from=start_from,
        page_delay=2,  # Segundos entre p√°ginas
        download_delay=1  # Segundos entre descargas
    )


if __name__ == "__main__":
    main()
