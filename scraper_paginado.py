#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sistema completo de scraping con paginaci√≥n
- Navega p√°gina por p√°gina
- Extrae URLs de cada p√°gina
- Descarga im√°genes autom√°ticamente
- Marca progreso en archivo
- Puede reanudar desde donde se qued√≥
"""
import os
import sys
import time
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime

sys.path.insert(0, os.path.dirname(__file__))
from download_property_images import PropertyImageDownloader


class PaginatedScraper:
    """Scraper con paginaci√≥n y tracking de progreso"""

    def __init__(self, progress_file="scraping_progress.json"):
        self.base_url = "https://bohioconsultores.com/resultados-de-busqueda/"
        self.progress_file = progress_file
        self.downloader = PropertyImageDownloader()
        self.progress = self.load_progress()

    def load_progress(self):
        """Carga progreso desde archivo JSON"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass

        # Progreso inicial
        return {
            'last_page': 0,
            'total_pages': 103,
            'processed_codes': [],
            'failed_codes': [],
            'stats': {
                'total_found': 0,
                'total_downloaded': 0,
                'total_images': 0,
                'total_failed': 0,
                'no_images': 0
            },
            'started_at': datetime.now().isoformat(),
            'last_updated': datetime.now().isoformat()
        }

    def save_progress(self):
        """Guarda progreso en archivo JSON"""
        self.progress['last_updated'] = datetime.now().isoformat()
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress, f, indent=2, ensure_ascii=False)
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è  Error guardando progreso: {e}")
            return False

    def fetch_page(self, page_num=1):
        """
        Obtiene una p√°gina de resultados
        NOTA: La paginaci√≥n puede ser por AJAX, necesitamos analizar el comportamiento
        """
        url = f"{self.base_url}?Servicio=2"

        # Si hay paginaci√≥n por par√°metro
        if page_num > 1:
            url += f"&page={page_num}"

        try:
            print(f"\nüìÑ Obteniendo p√°gina {page_num}/{self.progress['total_pages']}...")
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return None

    def extract_urls_from_page(self, html):
        """Extrae URLs de propiedades de una p√°gina"""
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        properties = []

        # Buscar todos los boxes de propiedades
        boxes = soup.find_all('div', class_='caja movil-50')

        for box in boxes:
            try:
                # Buscar enlace
                link = box.find('a', href=True)
                if not link or 'detalle-propiedad' not in link['href']:
                    continue

                url = link['href']
                if not url.startswith('http'):
                    url = f"https://bohioconsultores.com{url}"

                # Extraer c√≥digo
                code = self.downloader.extract_property_id_from_url(url)

                # Extraer imagen preview (opcional)
                img = box.find('img', src=True)
                preview_img = img['src'] if img else None

                # Extraer info adicional
                caption = box.find('div', class_='contenido')
                address = ""
                property_type = ""

                if caption:
                    addr_p = caption.find('p', class_='properties_address')
                    if addr_p:
                        address = addr_p.get_text(strip=True)

                    type_p = caption.find('p', class_='sc_properties_item_type')
                    if type_p:
                        property_type = type_p.get_text(strip=True)

                properties.append({
                    'code': code,
                    'url': url,
                    'address': address,
                    'type': property_type,
                    'preview_img': preview_img
                })

            except Exception as e:
                print(f"‚ö†Ô∏è  Error procesando box: {e}")
                continue

        return properties

    def is_already_processed(self, code):
        """Verifica si un c√≥digo ya fue procesado"""
        return code in self.progress['processed_codes']

    def mark_as_processed(self, code, success=True):
        """Marca un c√≥digo como procesado"""
        if success:
            if code not in self.progress['processed_codes']:
                self.progress['processed_codes'].append(code)
        else:
            if code not in self.progress['failed_codes']:
                self.progress['failed_codes'].append(code)

        self.save_progress()

    def download_property_images(self, property_data):
        """Descarga im√°genes de una propiedad"""
        code = property_data['code']
        url = property_data['url']

        print(f"\nüì• Propiedad: {code}")
        print(f"   Direcci√≥n: {property_data['address']}")
        print(f"   Tipo: {property_data['type']}")

        try:
            result = self.downloader.process_property_url(url, download_locally=True)

            if result and result.get('downloaded_images'):
                images_count = len(result['downloaded_images'])
                print(f"‚úÖ {images_count} im√°genes descargadas")

                self.progress['stats']['total_downloaded'] += 1
                self.progress['stats']['total_images'] += images_count
                self.mark_as_processed(code, success=True)
                return True

            elif result:
                print(f"‚ö†Ô∏è  Sin im√°genes")
                self.progress['stats']['no_images'] += 1
                self.mark_as_processed(code, success=True)
                return True

            else:
                print(f"‚ùå Error descargando")
                self.progress['stats']['total_failed'] += 1
                self.mark_as_processed(code, success=False)
                return False

        except Exception as e:
            print(f"‚ùå Error: {e}")
            self.progress['stats']['total_failed'] += 1
            self.mark_as_processed(code, success=False)
            return False

    def process_all_pages(self, start_page=None, max_pages=None):
        """
        Procesa todas las p√°ginas
        start_page: p√°gina desde donde empezar (None = continuar desde √∫ltima)
        max_pages: m√°ximo de p√°ginas a procesar (None = todas)
        """
        print("\n" + "="*80)
        print("SCRAPER PAGINADO - EXTRACCI√ìN Y DESCARGA COMPLETA")
        print("="*80)

        # Determinar p√°gina inicial
        if start_page is None:
            start_page = self.progress['last_page'] + 1

        if start_page > 1:
            print(f"\nüîÑ Continuando desde p√°gina {start_page}")

        print(f"\nüìä Progreso anterior:")
        print(f"   Procesadas: {len(self.progress['processed_codes'])}")
        print(f"   Fallidas: {len(self.progress['failed_codes'])}")
        print(f"   Total im√°genes: {self.progress['stats']['total_images']}")

        # IMPORTANTE: Por ahora, la paginaci√≥n parece ser por AJAX
        # Vamos a extraer de la primera p√°gina y procesar
        print("\n‚ö†Ô∏è  NOTA: La paginaci√≥n parece ser por AJAX.")
        print("   Procesando propiedades visibles en p√°gina actual...")

        # Obtener p√°gina
        html = self.fetch_page(page_num=1)

        if not html:
            print("‚ùå No se pudo obtener la p√°gina")
            return

        # Extraer propiedades
        properties = self.extract_urls_from_page(html)

        if not properties:
            print("‚ùå No se encontraron propiedades")
            return

        print(f"\n‚úÖ Encontradas {len(properties)} propiedades en la p√°gina")
        self.progress['stats']['total_found'] += len(properties)

        # Filtrar ya procesadas
        to_process = [p for p in properties if not self.is_already_processed(p['code'])]

        if not to_process:
            print("\n‚úÖ Todas las propiedades de esta p√°gina ya fueron procesadas")
            return

        print(f"üìã Por procesar: {len(to_process)}")

        # Procesar cada propiedad
        for idx, prop in enumerate(to_process, 1):
            print(f"\n[{idx}/{len(to_process)}] " + "="*60)

            self.download_property_images(prop)

            # Guardar progreso cada 5 propiedades
            if idx % 5 == 0:
                self.save_progress()
                print(f"\nüíæ Progreso guardado...")

            # Pausa entre descargas
            if idx < len(to_process):
                time.sleep(1)

        # Guardar progreso final
        self.progress['last_page'] = 1
        self.save_progress()

        # Reporte final
        self.print_final_report()

    def print_final_report(self):
        """Imprime reporte final"""
        print("\n" + "="*80)
        print("üìä REPORTE FINAL")
        print("="*80)

        stats = self.progress['stats']

        print(f"\nüìà ESTAD√çSTICAS:")
        print(f"   Propiedades encontradas: {stats['total_found']}")
        print(f"   ‚úÖ Descargadas exitosamente: {stats['total_downloaded']}")
        print(f"   ‚ö†Ô∏è  Sin im√°genes: {stats['no_images']}")
        print(f"   ‚ùå Fallidas: {stats['total_failed']}")
        print(f"   üì∏ Total im√°genes descargadas: {stats['total_images']}")

        print(f"\nüìã PROCESADAS:")
        print(f"   Total: {len(self.progress['processed_codes'])}")
        if self.progress['processed_codes']:
            print(f"   √öltimas 10: {', '.join(self.progress['processed_codes'][-10:])}")

        if self.progress['failed_codes']:
            print(f"\n‚ùå FALLIDAS ({len(self.progress['failed_codes'])}):")
            print(f"   C√≥digos: {', '.join(self.progress['failed_codes'][:20])}")

        print(f"\nüìÅ Archivos:")
        print(f"   Progreso: {self.progress_file}")
        print(f"   Im√°genes: property_images/")

        print("\n" + "="*80)

    def export_to_txt(self, output_file="processed_properties.txt"):
        """Exporta lista de propiedades procesadas a TXT"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("# PROPIEDADES PROCESADAS\n")
                f.write(f"# Total: {len(self.progress['processed_codes'])}\n")
                f.write(f"# Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("\n")

                f.write("## EXITOSAS\n")
                for code in self.progress['processed_codes']:
                    f.write(f"{code}\n")

                if self.progress['failed_codes']:
                    f.write("\n## FALLIDAS\n")
                    for code in self.progress['failed_codes']:
                        f.write(f"{code}\n")

            print(f"\n‚úÖ Lista exportada a: {output_file}")
            return True

        except Exception as e:
            print(f"‚ùå Error exportando: {e}")
            return False


def main():
    """Funci√≥n principal"""

    print("="*80)
    print("SCRAPER PAGINADO CON TRACKING DE PROGRESO")
    print("="*80)

    scraper = PaginatedScraper(progress_file="scraping_progress.json")

    # Verificar si hay progreso previo
    if scraper.progress['last_page'] > 0:
        print(f"\nüìã Progreso anterior detectado:")
        print(f"   √öltima p√°gina: {scraper.progress['last_page']}")
        print(f"   Procesadas: {len(scraper.progress['processed_codes'])}")
        print(f"   Im√°genes: {scraper.progress['stats']['total_images']}")

        response = input("\n¬øContinuar desde donde se qued√≥? (s/n): ")
        if response.lower() != 's':
            print("\nüîÑ Reiniciando progreso...")
            scraper.progress = scraper.load_progress()
            scraper.progress['last_page'] = 0
            scraper.progress['processed_codes'] = []
            scraper.save_progress()

    # Opciones de l√≠nea de comandos
    start_page = None
    max_pages = None

    if len(sys.argv) > 1:
        try:
            start_page = int(sys.argv[1])
        except:
            pass

    if len(sys.argv) > 2:
        try:
            max_pages = int(sys.argv[2])
        except:
            pass

    # Procesar p√°ginas
    scraper.process_all_pages(start_page=start_page, max_pages=max_pages)

    # Exportar lista
    scraper.export_to_txt("processed_properties.txt")


if __name__ == "__main__":
    main()
