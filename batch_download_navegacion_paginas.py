#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script para descargar im√°genes navegando p√°gina por p√°gina
Navega: p√°gina 1, 2, 3, 4, 5... hasta 105 o hasta que no haya m√°s resultados
"""
import os
import sys
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import unquote
import re
from download_property_images import PropertyImageDownloader


class PaginatedDownloader:
    """Descarga im√°genes navegando p√°gina por p√°gina en el sitio web"""

    def __init__(self, output_dir="property_images"):
        self.output_dir = output_dir
        self.downloader = PropertyImageDownloader(download_dir=output_dir)
        self.base_url = "https://bohioconsultores.com/resultados-de-busqueda/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

    def get_page_url(self, page_number, servicio=2):
        """
        Construye URL de p√°gina espec√≠fica
        p√°gina 1: ?Servicio=2
        p√°gina 2: ?Servicio=2&paged=2
        p√°gina 3: ?Servicio=2&paged=3
        """
        if page_number == 1:
            return f"{self.base_url}?Servicio={servicio}"
        else:
            return f"{self.base_url}?Servicio={servicio}&paged={page_number}"

    def fetch_page(self, page_number, servicio=2):
        """Obtiene HTML de una p√°gina espec√≠fica"""
        url = self.get_page_url(page_number, servicio)

        try:
            print(f"\nüì• Obteniendo p√°gina {page_number}...")
            print(f"   URL: {url}")

            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'

            print(f"   ‚úÖ Status: {response.status_code}")
            print(f"   üìÑ Tama√±o: {len(response.text)} caracteres")

            return response.text

        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            return None

    def extract_property_links(self, html):
        """Extrae enlaces de propiedades desde HTML"""
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')

        # Buscar enlaces con detalle-propiedad
        pattern_links = soup.find_all('a', href=re.compile(r'detalle-propiedad'))

        links = []
        seen = set()

        for link in pattern_links:
            href = link.get('href', '')
            if 'detalle-propiedad' in href:
                # Decodificar URL
                href_decoded = unquote(href)

                # Evitar duplicados
                if href_decoded not in seen:
                    seen.add(href_decoded)
                    links.append(href_decoded)

        return links

    def scrape_all_pages(self, servicio=2, max_pages=105, delay=2):
        """
        Navega todas las p√°ginas extrayendo enlaces

        Args:
            servicio: 1=Arriendo, 2=Venta
            max_pages: N√∫mero m√°ximo de p√°ginas a navegar (default: 105)
            delay: Segundos entre p√°ginas (default: 2)
        """
        print("\n" + "="*80)
        print("üîç NAVEGACI√ìN PAGINADA - BOHIO CONSULTORES")
        print("="*80)

        servicio_text = {1: "ARRIENDO", 2: "VENTA"}.get(servicio, "DESCONOCIDO")
        print(f"\n‚öôÔ∏è  Configuraci√≥n:")
        print(f"   Tipo: {servicio_text}")
        print(f"   P√°ginas m√°ximas: {max_pages}")
        print(f"   Delay entre p√°ginas: {delay}s")

        all_links = []
        page_number = 1
        consecutive_empty = 0  # Contador de p√°ginas vac√≠as consecutivas

        while page_number <= max_pages:
            print(f"\n{'='*80}")
            print(f"üìÑ P√ÅGINA {page_number}/{max_pages}")
            print(f"{'='*80}")

            # Obtener HTML de la p√°gina
            html = self.fetch_page(page_number, servicio)

            if not html:
                print(f"‚ö†Ô∏è  No se pudo obtener p√°gina {page_number}")
                consecutive_empty += 1
                if consecutive_empty >= 3:
                    print(f"\n‚ö†Ô∏è  3 p√°ginas consecutivas sin respuesta, deteniendo...")
                    break
                page_number += 1
                time.sleep(delay)
                continue

            # Extraer enlaces
            links = self.extract_property_links(html)

            print(f"\nüìä Propiedades encontradas: {len(links)}")

            if not links:
                consecutive_empty += 1
                print(f"‚ö†Ô∏è  P√°gina vac√≠a ({consecutive_empty}/3)")

                if consecutive_empty >= 3:
                    print(f"\n‚úÖ 3 p√°ginas consecutivas vac√≠as, fin de resultados alcanzado")
                    break
            else:
                consecutive_empty = 0  # Resetear contador

                # Filtrar links nuevos
                new_links = [link for link in links if link not in all_links]
                all_links.extend(new_links)

                print(f"   ‚úÖ Nuevas: {len(new_links)}")
                print(f"   üìä Total acumulado: {len(all_links)}")

                # Mostrar primeros 3 enlaces como muestra
                if new_links:
                    print(f"\n   Muestra de enlaces encontrados:")
                    for i, link in enumerate(new_links[:3], 1):
                        # Extraer c√≥digo
                        code_match = re.search(r'-(\d+)$', link)
                        code = code_match.group(1) if code_match else "?"
                        print(f"      {i}. C√≥digo {code}")

            # Siguiente p√°gina
            page_number += 1

            # Pausa entre p√°ginas
            if page_number <= max_pages:
                time.sleep(delay)

        print(f"\n{'='*80}")
        print(f"üìä RESUMEN DE NAVEGACI√ìN")
        print(f"{'='*80}")
        print(f"   P√°ginas navegadas: {page_number - 1}")
        print(f"   Total propiedades encontradas: {len(all_links)}")
        print(f"{'='*80}")

        return all_links

    def download_from_links(self, links, max_properties=None, start_from=0):
        """Descarga im√°genes de los enlaces extra√≠dos"""

        print("\n" + "="*80)
        print("üì• DESCARGA DE IM√ÅGENES")
        print("="*80)

        # Aplicar l√≠mites
        if start_from > 0:
            links = links[start_from:]
            print(f"\nüîÑ Iniciando desde posici√≥n: {start_from}")

        if max_properties:
            links = links[:max_properties]
            print(f"üî¢ Limitando a: {max_properties} propiedades")

        print(f"\nüöÄ Procesando {len(links)} propiedades...")

        # Estad√≠sticas
        stats = {
            'total': len(links),
            'success': 0,
            'failed': 0,
            'no_images': 0,
            'total_images': 0,
            'failed_urls': []
        }

        # Procesar cada enlace
        for idx, url in enumerate(links, 1):
            print(f"\n{'='*80}")
            print(f"[{idx}/{len(links)}] Procesando propiedad")
            print(f"{'='*80}")

            # Extraer c√≥digo
            code_match = re.search(r'-(\d+)$', url)
            code = code_match.group(1) if code_match else "N/A"

            print(f"C√≥digo: {code}")
            print(f"URL: {url}")

            try:
                # Descargar im√°genes
                result = self.downloader.process_property_url(url, download_locally=True)

                if result and result.get('downloaded_images'):
                    stats['success'] += 1
                    stats['total_images'] += len(result['downloaded_images'])
                    print(f"‚úÖ {len(result['downloaded_images'])} im√°genes descargadas")
                elif result:
                    stats['no_images'] += 1
                    print(f"‚ö†Ô∏è  Sin im√°genes")
                else:
                    stats['failed'] += 1
                    stats['failed_urls'].append(url)
                    print(f"‚ùå Error procesando")

            except Exception as e:
                stats['failed'] += 1
                stats['failed_urls'].append(url)
                print(f"‚ùå Error: {e}")

            # Pausa entre propiedades
            if idx < len(links):
                time.sleep(1)

        # Reporte final
        print("\n" + "="*80)
        print("üìä REPORTE FINAL")
        print("="*80)
        print(f"   Total Procesadas: {stats['total']}")
        print(f"   ‚úÖ Exitosas: {stats['success']}")
        print(f"   ‚ùå Fallidas: {stats['failed']}")
        print(f"   ‚ö†Ô∏è  Sin im√°genes: {stats['no_images']}")
        print(f"   üì∏ Total Im√°genes Descargadas: {stats['total_images']}")

        if stats['failed_urls']:
            print(f"\nüìã URLs Fallidas ({len(stats['failed_urls'])}):")
            for i, url in enumerate(stats['failed_urls'][:10], 1):
                print(f"   {i}. {url}")
            if len(stats['failed_urls']) > 10:
                print(f"   ... y {len(stats['failed_urls']) - 10} m√°s")

        print("="*80)

        return stats

    def run_full_process(self, servicio=2, max_pages=105, max_properties=None,
                        start_from=0, page_delay=2, download_delay=1):
        """
        Proceso completo: navega p√°ginas y descarga im√°genes
        """
        print("\n" + "="*80)
        print("üöÄ PROCESO COMPLETO - NAVEGACI√ìN Y DESCARGA")
        print("="*80)

        # Fase 1: Navegar p√°ginas y extraer enlaces
        links = self.scrape_all_pages(
            servicio=servicio,
            max_pages=max_pages,
            delay=page_delay
        )

        if not links:
            print("\n‚ùå No se encontraron propiedades")
            return None

        # Guardar enlaces en archivo
        servicio_name = {1: "arriendo", 2: "venta"}.get(servicio, "todas")
        links_file = os.path.join(self.output_dir, f"property_links_{servicio_name}_paginated.txt")
        os.makedirs(self.output_dir, exist_ok=True)

        with open(links_file, 'w', encoding='utf-8') as f:
            for link in links:
                f.write(f"{link}\n")

        print(f"\nüíæ Enlaces guardados en: {links_file}")

        # Fase 2: Descargar im√°genes
        stats = self.download_from_links(
            links=links,
            max_properties=max_properties,
            start_from=start_from
        )

        print("\n" + "="*80)
        print("‚úÖ PROCESO COMPLETADO")
        print("="*80)

        return {
            'links': links,
            'stats': stats
        }


def main():
    """Funci√≥n principal"""

    print("="*80)
    print("DESCARGADOR CON NAVEGACI√ìN PAGINADA")
    print("="*80)

    # Configuraci√≥n
    servicio = 2  # 1=Arriendo, 2=Venta
    max_pages = 105  # M√°ximo de p√°ginas a navegar
    max_properties = None  # None = todas
    start_from = 0  # √çndice de inicio

    # Parsear argumentos
    if len(sys.argv) > 1:
        try:
            arg1 = sys.argv[1].lower()
            if arg1 == 'venta':
                servicio = 2
            elif arg1 == 'arriendo':
                servicio = 1
            else:
                max_pages = int(sys.argv[1])
        except:
            print("\nüí° USO:")
            print("   python batch_download_navegacion_paginas.py [tipo] [max_paginas] [max_propiedades]")
            print("\nüí° EJEMPLOS:")
            print("   python batch_download_navegacion_paginas.py venta")
            print("   python batch_download_navegacion_paginas.py venta 10")
            print("   python batch_download_navegacion_paginas.py venta 10 50")
            print("   python batch_download_navegacion_paginas.py arriendo 105")
            print("\nüìù Tipos: venta, arriendo")
            print("üìù Por defecto navega hasta p√°gina 105 o hasta que no haya m√°s resultados")
            return

    if len(sys.argv) > 2:
        try:
            max_pages = int(sys.argv[2])
        except:
            pass

    if len(sys.argv) > 3:
        try:
            max_properties = int(sys.argv[3])
        except:
            pass

    # Crear descargador
    downloader = PaginatedDownloader()

    # Ejecutar proceso completo
    downloader.run_full_process(
        servicio=servicio,
        max_pages=max_pages,
        max_properties=max_properties,
        start_from=start_from,
        page_delay=2,  # 2 segundos entre p√°ginas
        download_delay=1  # 1 segundo entre descargas
    )


if __name__ == "__main__":
    main()
